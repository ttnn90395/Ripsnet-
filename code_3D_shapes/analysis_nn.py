# Analysis of RN
#    comparison between calculation times of Gudhi and RN,
#    comparison between classification results on true and predicted PIs.


import matplotlib.pyplot as plt
import dill as pck
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import tensorflow as tf
from tensorflow.keras import regularizers, layers
import pandas as pd
from gudhi.representations import PersistenceImage, DiagramSelector, Landscape
from scipy.spatial import distance
from time import time
import sys
import os
import argparse
from pathlib import Path
import xgboost as xgb
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import LabelEncoder

from helper_fctns.get_names import get_dirs_results, get_dir_model, get_dir_data, get_suffix_dataset


parser = argparse.ArgumentParser()
parser.add_argument(
    '-d',
    '--data',
    required=False,
    type=str,
    default='ucr_timeseries',
    help='Which data to use. Can be in ["ucr_timeseries", "modelnet"]'
)
parser.add_argument(
    '-md',
    '--model_dir',
    required=False,
    type=str,
    default='',
    help='Directory of the trained RipsNet.'
)
parser.add_argument(
    '-mn',
    '--model_name',
    required=False,
    type=str,
    help='Model name.',
    default=''
)
parser.add_argument(
    '-f',
    '--fold',
    required=False,
    type=int,
    default=0,
    help='Fold number when generating multiple data sets.'
)
parser.add_argument(
    '-tsn',
    '--time_series_name',
    required=False,
    type=str,
    default='',
    help='Name of the UCR time series.'
)
parser.add_argument(
    '-dPIp',
    '--data_PI_par',
    required=False,
    type=str,
    help='Dataset of PI parameters.',
    default=''
)
parser.add_argument(
    '-dtrn',
    '--data_train',
    required=False,
    type=str,
    help='Training dataset',
    default=''
)
parser.add_argument(
    '-dtst',
    '--data_test',
    required=False,
    type=str,
    help='Test dataset',
    default=''
)
parser.add_argument(
    '-n',
    '--normalize',
    required=False,
    action = 'store_true',
    help='Control whether to normalize or not.',
)
parser.add_argument(
    '-b',
    '--bulk',
    required=False,
    action = 'store_true',
    help='Control whether to analyze all models in the model directory "bulk"-wise.',
)
parser.add_argument(
    '-do',
    '--dropout',
    type=float,
    required=False,
    default=0,
    help='Dropout used to train the rips net.',
)
parser.add_argument(
    '-reg',
    '--regularization',
    type=float,
    required=False,
    default=0,
    help='l2 regularization used to train the rips net.',
)
parser.add_argument(
    '-bs',
    '--batch_size',
    type=int,
    required=False,
    default=32,
    help='Batch size used to train the rips net.',
)
# parser.add_argument(
#     '-cl',
#     '--num_classes',
#     required=False,
#     type=int,
#     default=3,
#     help='Number of classes of the classification task.',
# )
parser.add_argument(
    '-td',
    '--tde_dim',
    required=False,
    type=int,
    default=3,
    help='Time delay embedding dimension.'
)
parser.add_argument(
    '-hd',
    '--homdims',
    required=False,
    nargs='+',
    type=int,
    default=[1],
    help='List of homology dimensions.'
)

parser.add_argument(
    '-r',
    '--representation',
    required=False,
    type=str,
    default='PI',
    help='Choice of the persistence diagram representation; persistence image/landscape. Must be in: ["PI", "LS"].',
)
parser.add_argument(
    '-mnc',
    '--model_net_choice',
    required=False,
    type=int,
    default=10,
    help='Choice between modelnet40 and modelnet10. Must be in [10, 40]',
)

def create_model(num_classes=3, shape=2500, regularization=0, dropout=0):
    inputs = tf.keras.Input(shape=shape)
    x = tf.keras.layers.Dense(100, activation='relu', kernel_regularizer=regularizers.l2(regularization))(inputs)
    x = tf.keras.layers.Dense(50, activation='relu', kernel_regularizer=regularizers.l2(regularization))(x)
    outputs = tf.keras.layers.Dense(num_classes, kernel_regularizer=regularizers.l2(regularization))(x)
    model_classif = tf.keras.Model(inputs=inputs, outputs=outputs)
    model_classif.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
    return model_classif


def create_model(num_classes=3, shape=2500, regularization=0, dropout=0):
    class ModelClassif(nn.Module):
        def __init__(self):
            super(ModelClassif, self).__init__()
            self.regularization = regularization
            self.dropout = dropout

            self.fc1 = nn.Linear(shape, 100)
            self.fc2 = nn.Linear(100, 50)
            self.fc3 = nn.Linear(50, num_classes)

            self.relu = nn.ReLU()
            self.dropout_layer = nn.Dropout(p=dropout)

        def forward(self, x):
            x = self.relu(self.fc1(x))
            if self.dropout > 0:
                x = self.dropout_layer(x)

            x = self.relu(self.fc2(x))
            if self.dropout > 0:
                x = self.dropout_layer(x)

            x = self.fc3(x)  # no activation â†’ logits (from_logits=True)
            return x

    model_classif = ModelClassif()
    return model_classif

def DTW(a, b):
    an = a.size
    bn = b.size
    pointwise_distance = distance.cdist(a.reshape(-1,1),b.reshape(-1,1))
    cumdist = np.matrix(np.ones((an+1,bn+1)) * np.inf)
    cumdist[0,0] = 0
    for ai in range(an):
        for bi in range(bn):
            minimum_cost = np.min([cumdist[ai, bi+1], cumdist[ai+1, bi], cumdist[ai, bi]])
            cumdist[ai+1, bi+1] = pointwise_distance[ai,bi] + minimum_cost
    return cumdist[an, bn]

# Weight function dictionary (workaround for pickling error):
weight_dict = {'constant': lambda x: 1,
               'quadratic': lambda x: x[1] ** 2,
               'linear': lambda x: x[1],
               'tanh': lambda x: 10 * np.tanh(x[1]),
               'quintic': lambda x: x[1] ** 5,
               }

# Check if the execution is on the cluster or not:
on_cluster = False

args = parser.parse_args()

data_name = args.data
model_name = args.model_name
time_series_name = args.time_series_name
dataset_vectorization_params = args.data_PI_par
dataset_train_name = args.data_train
dataset_test_name = args.data_test
normalize = args.normalize
bulk = args.bulk
tdedim = args.tde_dim
homdims = args.homdims
representation = args.representation
model_net_choice = args.model_net_choice
fold = args.fold
dropout = args.dropout
batch_size = args.batch_size
regularization = args.regularization

## Make sure we always have a list of homology dimensions:
if isinstance(homdims, int):
    homdims = [homdims]

num_homdims = len(homdims)

model_net = False
assert(data_name in ['ucr_timeseries', 'modelnet'])
if data_name == 'modelnet':
    model_net = True
    data_name = data_name + str(model_net_choice)


assert(bulk or model_name != '')

# GPU support:
use_GPU = True
if use_GPU:
    config = tf.compat.v1.ConfigProto()
    config.gpu_options.allow_growth = True
    tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))

    # physical_devices = tf.config.experimental.list_physical_devices('GPU')
    # tf.config.experimental.set_memory_growth(physical_devices[0], True)
else:
    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'

#Set the data directories:
cwd = os.getcwd()
data_base_dir = os.path.join(cwd, 'datasets/saved_datasets')
model_base_dir = os.path.join(cwd, f'models/{representation}')
model_dir = os.path.join(model_base_dir, data_name)
results_dir = os.path.join(cwd, f'results/{representation}/{data_name}')

vectorization_param_data_dir    = os.path.join(data_base_dir, 'rn_train')
ml_train_dataset_dir = os.path.join(data_base_dir, 'ml_train')
test_dataset_dir     = os.path.join(data_base_dir, 'test')
figures_output_dir   = os.path.join(results_dir, 'analysis', 'figures', time_series_name)
Path(vectorization_param_data_dir).mkdir(parents=True, exist_ok=True)
Path(ml_train_dataset_dir).mkdir(parents=True, exist_ok=True)
Path(test_dataset_dir).mkdir(parents=True, exist_ok=True)
Path(figures_output_dir).mkdir(parents=True, exist_ok=True)

# If the names of the datasets are not specified, automatically load the corresponding datasets:
if dataset_train_name == '' or dataset_test_name == '':
    assert(dataset_vectorization_params == '')

if dataset_vectorization_params != '':
    vectorization_param_ds_name = os.path.join(vectorization_param_data_dir, dataset_vectorization_params + '.pkl')
else:
    vectorization_param_ds_name = pck.load(open(os.path.join(model_dir, model_name, 'model_train_dataset_filename.pkl'), 'rb'))
if dataset_train_name != '':
    dataset_train_name = os.path.join(ml_train_dataset_dir, dataset_train_name + '.pkl')
else:
    dataset_train_name = vectorization_param_ds_name.replace('/rn_train/', '/ml_train/')
if dataset_test_name != '':
    dataset_test_name = os.path.join(test_dataset_dir, dataset_test_name + '.pkl')
else:
    dataset_test_name = vectorization_param_ds_name.replace('/rn_train/', '/test/')

vectorization_param_ds_name_name_simple = os.path.splitext(os.path.split(vectorization_param_ds_name)[1])[0]
dataset_train_name_simple = os.path.splitext(os.path.split(dataset_train_name)[1])[0]
dataset_test_name_simple = os.path.splitext(os.path.split(dataset_test_name)[1])[0]

results_file = os.path.join(results_dir, 'analysis_results.csv')

# Load the NN
if bulk:
    model_names = os.listdir(model_dir)
else:
    model_names = [model_name]

for model_name in model_names:
    model = tf.keras.models.load_model(os.path.join(model_dir, model_name))
    model.summary()
    #SVG(tf.keras.utils.model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))
    #tf.keras.utils.plot_model(model, to_file='dot_img_file.jpg', show_shapes=True, rankdir='LR')

    print('Dataset used to load the vectorization parameters (same as training of the ripsnet): ', vectorization_param_ds_name, '\n')

    ## Data used for the training of the ML model.
    print('Dataset used for the training of the ML model: ', dataset_train_name, '\n')
    data_trn = pck.load(open(dataset_train_name, 'rb'))
    label_train_ml_train          = data_trn["label_train"]
    label_test_ml_train           = data_trn["label_test"]
    pc_train_ml_train             = data_trn["data_train"]
    pc_test_ml_train              = data_trn['data_test']
    vect_gudhi_train_ml_train     = data_trn["vectorization_train"]
    vect_test_gudhi_ml_train      = data_trn['vectorization_test']
    vect_gudhi_train_ml_train_DTM = data_trn['vectorization_train_DTM']
    vect_test_gudhi_ml_train_DTM  = data_trn['vectorization_test_DTM']
    num_classes                   = data_trn['num_classes']
    scaling_factor                = data_trn['scaling_factor']

    if model_net:
        time_Gudhi_train_train = data_trn['time_Gudhi_train']
        time_DTM_train_train   = data_trn['time_DTM_train']
        normalize_pc           = data_trn['normalize_pc']
        sample_even            = data_trn['sample_even']
        num_pts                = data_trn['num_pts']

    if not model_net:
        len_TS = data_trn['TS length']
        skip = data_trn['skip']
        skipauto = data_trn['skipauto']
        delay = data_trn['delay']
        maxd_train = data_trn['RC_max_dist']
        ts_classif_train = np.vstack(data_trn["ts_train"])
    assert(representation == data_trn['representation'])
    assert(representation in ['PI', 'LS'])

    ## Noisy data used for the evaluation of the Ml models.
    print('(Noisy) Test dataset for evaluation of the models: ', dataset_test_name, '\n')
    data_test = pck.load(open(dataset_test_name, 'rb'))
    label_test_ml_eval                = data_test["label_test"]
    pc_test_ml_eval                   = data_test["data_test"]
    pc_test_ml_eval_clean             = data_test["data_test_clean"]
    vect_gudhi_test_ml_eval           = data_test['vectorization_test']
    vect_gudhi_test_ml_eval_clean     = data_test['vectorization_test_clean']
    vect_gudhi_test_ml_eval_DTM       = data_test['vectorization_test_DTM']
    vect_gudhi_test_ml_eval_clean_DTM = data_test['vectorization_test_clean_DTM']
    pct_noise                         = data_test['pct_noise']

    if model_net:
        time_Gudhi_test_ml_eval = data_test['time_Gudhi_test']
        time_DTM_test_ml_eval   = data_test['time_DTM_test']

    if not model_net:
        maxd_test = data_test['RC_max_dist']
        ts_classif_test = np.vstack(data_test["ts_test"])
        assert(len_TS       == data_test["TS length"])
        assert(skip         == data_test['skip'])
        assert (skipauto    == data_test['skipauto'])
        assert(delay        == data_test['delay'])

    assert (representation == data_test['representation'])
    assert (scaling_factor == data_test['scaling_factor'])
    assert (num_classes == data_test['num_classes'])

    if representation == 'PI':
        print(homdims)
        print(data_trn['vectorization_params'])
        vparams = data_trn['vectorization_params'][homdims[0]]
        PI_size = vparams['resolution'][0]
        vectorization_size = num_homdims * vparams['resolution'][0] * vparams['resolution'][1]
    else:
        vparams = data_trn['vectorization_params'][homdims[0]]
        num_landscapes = vparams['num_landscapes']
        resolution = vparams['resolution']
        vectorization_size = num_homdims * num_landscapes * resolution

    # PI_size   = int(np.sqrt(vect_train_gudhi_ml_train.shape[1]))

    # workaround for modelnet:
    if model_net:
        pc_train_ml_train = [pc_train_ml_train[i, :, :] for i in range(0, pc_train_ml_train.shape[0])]
        pc_test_ml_eval = [pc_test_ml_eval[i, :, :] for i in range(0, pc_test_ml_eval.shape[0])]

    data_sets_train_ml_train_test_ml_eval = pc_train_ml_train + pc_test_ml_eval
    N_sets    = len(pc_train_ml_train) + len(pc_test_ml_eval)


#######################################################################33
    # Plot the points clouds

    fig = plt.figure()
    fig.set_size_inches(6, 4.5)
    for i in range(9):
        ax = fig.add_subplot(3,3,i+1, projection='3d') #plt.subplot(3, 3, i + 1)
        x = data_sets_train_ml_train_test_ml_eval[len(pc_train_ml_train) + i][:, 0]
        y = data_sets_train_ml_train_test_ml_eval[len(pc_train_ml_train) + i][:, 1]
        z = data_sets_train_ml_train_test_ml_eval[len(pc_train_ml_train) + i][:, 2]
        ax.scatter(x, y, z, s=0.5)
        ax.axis('off')
        ax.title.set_text(label_test_ml_eval[i])
    plt.suptitle('Point clouds on (noisy) test data')
    # plt.savefig(os.path.join(figures_output_dir, dataset_test_name_simple + '_point_clouds_on_test.jpg'),
    #             dpi=200)
    plt.savefig(os.path.join(figures_output_dir, f'point_clouds_on_test_pct_noise_{pct_noise}.jpg'),
                dpi=200)

    fig = plt.figure()
    fig.set_size_inches(6, 4.5)
    for i in range(9):
        ax = fig.add_subplot(3,3,i+1, projection='3d') #plt.subplot(3, 3, i + 1)
        # x = data_sets_train_ml_train_test_ml_eval[i][:, 0]
        # y = data_sets_train_ml_train_test_ml_eval[i][:, 1]
        # z = data_sets_train_ml_train_test_ml_eval[i][:, 2]
        x = pc_test_ml_eval_clean[i][:, 0]
        y = pc_test_ml_eval_clean[i][:, 1]
        z = pc_test_ml_eval_clean[i][:, 2]
        ax.scatter(x, y, z, s=0.5)
        ax.axis('off')
        # ax.title.set_text(label_train_ml_train[i])
        ax.title.set_text(label_test_ml_eval[i])
    plt.suptitle('Point clouds on (clean) test data')
    # plt.savefig(os.path.join(figures_output_dir, dataset_test_name_simple + '_point_clouds_on_test_clean.jpg'),
    #             dpi=200)
    plt.savefig(os.path.join(figures_output_dir, f'point_clouds_on_test_clean.jpg'),
                dpi=200)

    dataset_rn_train = pck.load(open(vectorization_param_ds_name, 'rb'))
    vect_params_rn_train  = dataset_rn_train['vectorization_params'][homdims[0]]
    vect_test_rn_train = dataset_rn_train['vectorization_test'] #PIs generated from clean data
    label_test_rn_train         = dataset_rn_train['label_test']



    # Set the max distance parameter for the Rips complex:
    if not model_net:
        maxd_RC = dataset_rn_train['RC_max_dist']
        # maxd = np.min([maxd_PI, maxd_train, maxd_test])
        maxd = maxd_RC

    if representation == 'PI':
        wgt_name = vect_params_rn_train['weight']
        vect_params_rn_train['weight'] = weight_dict[vect_params_rn_train['weight']]  # weight_dict[weight_names[0]]
        vectorization = PersistenceImage(**vect_params_rn_train)
        vect_gudhi_ml_train_ml_eval = np.zeros((N_sets, vectorization_size))


    else:
        print('\nvectorization parameters: ', vect_params_rn_train)
        vectorization = Landscape(**vect_params_rn_train)

    if normalize:
        for hdidx in range(num_homdims):
            st_idx  = hdidx * int(vectorization_size / num_homdims)
            end_idx = (hdidx + 1) * int(vectorization_size / num_homdims)

            MV = np.max(vect_gudhi_train_ml_train[:, st_idx:end_idx])
            vect_gudhi_train_ml_train[:, st_idx:end_idx] /= MV
            MV = np.max(vect_gudhi_test_ml_eval_clean[:, st_idx:end_idx])
            vect_gudhi_test_ml_eval_clean[:, st_idx:end_idx] /= MV
            # MV = np.max(vect_gudhi_test_ml_eval[:, st_idx:end_idx])
            vect_gudhi_test_ml_eval[:, st_idx:end_idx] /= MV

            MV = np.max(vect_gudhi_train_ml_train_DTM[:, st_idx:end_idx])
            vect_gudhi_train_ml_train_DTM[:, st_idx:end_idx] /= MV
            MV = np.max(vect_gudhi_test_ml_eval_clean_DTM[:, st_idx:end_idx])
            vect_gudhi_test_ml_eval_clean_DTM[:, st_idx:end_idx] /= MV
            # MV = np.max(vect_gudhi_test_ml_eval[:, st_idx:end_idx])
            vect_gudhi_test_ml_eval_DTM[:, st_idx:end_idx] /= MV


    if model_net:
        vect_gudhi_ml_train_ml_eval = np.concatenate((vect_gudhi_train_ml_train, vect_gudhi_test_ml_eval), axis=0)
    else:
        vect_gudhi_ml_train_ml_eval = vect_gudhi_train_ml_train + vect_gudhi_test_ml_eval


    timeG = time_Gudhi_test_ml_eval + time_Gudhi_train_train
    print('Time taken by Gudhi = {} seconds\n'.format(timeG))
    timeDTM = time_DTM_test_ml_eval + time_DTM_train_train
    print('Time taken by DTM = {} seconds\n'.format(timeDTM))

    # Plot the true PIs
    if representation == 'PI':
        reshape_target_size = [num_homdims * PI_size, PI_size]
    else:
        reshape_target_size = [num_landscapes, num_homdims * resolution]

    plt.figure()
    for i in range(9):
        plt.subplot(3, 3, i + 1)
        if representation == 'PI':
            plt.imshow(np.flip(np.reshape(vect_gudhi_ml_train_ml_eval[i], reshape_target_size), 0), vmin=0, vmax=1, cmap='jet')
            plt.suptitle('The true GUDHI PI on clean train')
        else:
            for landscape_id in vect_gudhi_ml_train_ml_eval[i].reshape(reshape_target_size):
                plt.plot(landscape_id)
            plt.suptitle('The true GUDHI LS on clean train')

    plt.savefig(os.path.join(figures_output_dir, dataset_test_name_simple + '_true_vect_on_train.jpg'))

    plt.figure()
    for i in range(9):
        plt.subplot(3, 3, i + 1)
        if representation == 'PI':
            plt.imshow(np.flip(np.reshape(vect_gudhi_train_ml_train_DTM[i], reshape_target_size), 0), vmin=0, vmax=1, cmap='jet')
            plt.suptitle('The DTM PI on clean train')
        else:
            for landscape_id in vect_gudhi_train_ml_train_DTM[i].reshape(reshape_target_size):
                plt.plot(landscape_id)
            plt.suptitle('The DTM LS on clean train')

    plt.savefig(os.path.join(figures_output_dir, dataset_test_name_simple + '_DTM_vect_on_train.jpg'))


    plt.figure()
    for i in range(9):
        plt.subplot(3, 3, i + 1)
        if representation == 'PI':
            plt.imshow(np.flip(np.reshape(vect_gudhi_test_ml_eval_clean[i], reshape_target_size), 0), vmin=0, vmax=1, cmap='jet')
            plt.suptitle('The corresponding true GUDHI PI on clean test')
        else:
            for landscape_id in vect_gudhi_test_ml_eval_clean[i].reshape(reshape_target_size):
                plt.plot(landscape_id)
            plt.suptitle('The corresponding true GUDHI LS on clean test')
    plt.savefig(os.path.join(figures_output_dir, dataset_test_name_simple + '_true_vect_on_clean_test.jpg'))

    plt.figure()
    for i in range(9):
        plt.subplot(3, 3, i + 1)
        if representation == 'PI':
            plt.imshow(np.flip(np.reshape(vect_gudhi_test_ml_eval_clean_DTM[i], reshape_target_size), 0), vmin=0, vmax=1, cmap='jet')
            plt.suptitle('The corresponding DTM PI on clean test')
        else:
            for landscape_id in vect_gudhi_test_ml_eval_clean_DTM[i].reshape(reshape_target_size):
                plt.plot(landscape_id)
            plt.suptitle('The corresponding DTM LS on clean test')
    plt.savefig(os.path.join(figures_output_dir, dataset_test_name_simple + '_DTM_vect_on_clean_test.jpg'))


    plt.figure()
    for i in range(9):
        plt.subplot(3, 3, i + 1)
        if representation == 'PI':
            plt.imshow(np.flip(np.reshape(vect_gudhi_ml_train_ml_eval[len(pc_train_ml_train) + i], reshape_target_size), 0), vmin=0, vmax=1, cmap='jet')
            plt.suptitle('The true GUDHI PI on noisy test')
        else:
            for landscape_id in vect_gudhi_ml_train_ml_eval[len(pc_train_ml_train) + i].reshape(reshape_target_size):
                plt.plot(landscape_id)
            plt.suptitle('The true GUDHI LS on noisy test')
    plt.savefig(os.path.join(figures_output_dir, dataset_test_name_simple + '_true_vect_on_noisy_test.jpg'))

    plt.figure()
    for i in range(9):
        plt.subplot(3, 3, i + 1)
        if representation == 'PI':
            plt.imshow(np.flip(np.reshape(vect_gudhi_test_ml_eval_DTM[i], reshape_target_size), 0), vmin=0, vmax=1, cmap='jet')
            plt.suptitle('The DTM PI on noisy test')
        else:
            for landscape_id in vect_gudhi_test_ml_eval_DTM[i].reshape(reshape_target_size):
                plt.plot(landscape_id)
            plt.suptitle('The DTM LS on noisy test')
    plt.savefig(os.path.join(figures_output_dir, dataset_test_name_simple + '_DTM_vect_on_noisy_test.jpg'))

    # Compute their PIs with the NN and save computation time

    data_sets_train_ml_train_test_ml_eval = tf.ragged.constant([[list(c) for c in list(data_sets_train_ml_train_test_ml_eval[i])] for i in range(len(data_sets_train_ml_train_test_ml_eval))], ragged_rank=1)
    data_test_ml_eval_clean = tf.ragged.constant([[list(c) for c in list(pc_test_ml_eval_clean[i])] for i in range(len(pc_test_ml_eval_clean))], ragged_rank=1)
    vect_RN_test_ml_eval_clean = model.predict(data_test_ml_eval_clean)
    data_test_ml_train = tf.ragged.constant([[list(c) for c in list(pc_test_ml_train[i])] for i in range(len(pc_test_ml_train))], ragged_rank=1)
    vect_RN_test_ml_train = model.predict(data_test_ml_train)
    starttimeRN = time()
    vect_RN_train_ml_train_test_ml_eval = model.predict(data_sets_train_ml_train_test_ml_eval)
    timeRN = time() - starttimeRN
    print('Time taken by RN = {} seconds \n'.format(timeRN))

    # Plot the predicted PIs

    plt.figure()
    for i in range(9):
        plt.subplot(3, 3, i + 1)
        if representation == 'PI':
            plt.imshow(np.flip(np.reshape(vect_RN_train_ml_train_test_ml_eval[len(pc_train_ml_train) + i], reshape_target_size), 0), vmin=0, vmax=1, cmap='jet')
            plt.suptitle('The corresponding predicted PI by RN on (noisy) test')
        else:
            for landscape_id in vect_RN_train_ml_train_test_ml_eval[len(pc_train_ml_train) + i].reshape(reshape_target_size):  # TODO: uber dirty.
                plt.plot(landscape_id)
            plt.suptitle('The corresponding predicted LS by RN on (noisy) test')
    plt.savefig(os.path.join(figures_output_dir, dataset_test_name_simple + '_predicted_vect_on_test.jpg'))



    # Compute the MSE between the true and predicted PIs

    mse = (np.square(vect_RN_train_ml_train_test_ml_eval - vect_gudhi_ml_train_ml_eval)).mean(axis=None)

    # We test if the PIs computed with the NN and the PIs computed with Gudhi have the same distribution with Kolmogorov-Smirnov test, i.e., (H_0): D_NN = D_G

    vect_RN_train_ml_train, vect_RN_test_ml_eval = vect_RN_train_ml_train_test_ml_eval[:len(pc_train_ml_train)], vect_RN_train_ml_train_test_ml_eval[len(pc_train_ml_train):]


###################################################################

    N_sets_train = len(pc_train_ml_train)
    N_sets_test = len(pc_test_ml_eval)


    print("N_sets_train :       ", N_sets_train)
    print("N_sets_test :        ", N_sets_test)
    print("vectorization_size : ", vectorization_size)
    print("Number of classes:   ", num_classes)

    le = LabelEncoder().fit(np.concatenate([label_train_ml_train, label_test_ml_eval]))
    label_train_ml_train = le.transform(label_train_ml_train)
    label_test_ml_eval  = le.transform(label_test_ml_eval)
    label_test_ml_train = le.transform(label_test_ml_train)

    ############################ non-baseline models ##################################
    # Fit the classification model model_classif_gudhi and DTM
    model_classif_trained_on_gudhi = create_model(num_classes=num_classes, shape=vectorization_size)

    history_classif_trained_on_gudhi = model_classif_trained_on_gudhi.fit(vect_gudhi_train_ml_train,
                                                                          label_train_ml_train,
                                                                          epochs=500,
                                                                          verbose=0)
    eval_loss_NN_gudhi_gudhi, eval_acc_NN_gudhi_gudhi = model_classif_trained_on_gudhi.evaluate(vect_gudhi_test_ml_eval,
                                                                                                label_test_ml_eval,
                                                                                                verbose=2)
    eval_loss_NN_gudhi_RN, eval_acc_NN_gudhi_RN = model_classif_trained_on_gudhi.evaluate(vect_RN_test_ml_eval,
                                                                                                label_test_ml_eval,
                                                                                                verbose=2)
    eval_loss_NN_gudhi_clean, eval_acc_NN_gudhi_clean_gudhi = model_classif_trained_on_gudhi.evaluate(vect_gudhi_test_ml_eval_clean,
                                                                                                      label_test_ml_eval,
                                                                                                      verbose=2)
    print('\nTest accuracy of NN trained on clean GUDHI vectorization and evaluated on clean GUDHI vectorization: ',
          eval_acc_NN_gudhi_clean_gudhi)
    print('\nTest accuracy of NN trained on clean GUDHI vect evaluated on noisy GUDHI vect:', eval_acc_NN_gudhi_gudhi)
    print('\nTest accuracy of NN trained on clean GUDHI vect evaluated on noisy RN vect:', eval_acc_NN_gudhi_RN)

    model_classif_trained_on_DTM = create_model(num_classes=num_classes, shape=vectorization_size)

    history_classif_trained_on_DTM = model_classif_trained_on_DTM.fit(vect_gudhi_train_ml_train_DTM,
                                                                      label_train_ml_train, epochs=500,
                                                                      verbose=0)
    eval_loss_NN_DTM_DTM, eval_acc_NN_DTM_DTM = model_classif_trained_on_DTM.evaluate(vect_gudhi_test_ml_eval_DTM,
                                                                                      label_test_ml_eval,
                                                                                      verbose=2)

    eval_loss_NN_DTM_clean, eval_acc_NN_DTM_clean_DTM = model_classif_trained_on_DTM.evaluate(vect_gudhi_test_ml_eval_clean_DTM,
                                                                                              label_test_ml_eval,
                                                                                              verbose=2)
    print('\nTest accuracy of NN trained on clean DTM vectorization and evaluated on clean DTM vectorization: ',
          eval_acc_NN_DTM_clean_DTM)
    print('\nTest accuracy of NN trained on clean DTM vect evaluated on noisy DTM vect:', eval_acc_NN_DTM_DTM)


    # Fit the classification model model_classif_NN

    model_classif_trained_on_RN = create_model(num_classes=num_classes, shape=vectorization_size)

    history_classif_trained_on_RN = model_classif_trained_on_RN.fit(vect_RN_train_ml_train, label_train_ml_train,
                                                                    epochs=500,
                                                                    validation_data=(vect_RN_test_ml_train, label_test_ml_train),
                                                                    verbose=0)#, validation_data=(vect_test_gudhi_ml_train, label_test_ml_train))#, validation_data=(vect_RN_ml_eval, label_test_ml_eval))
    eval_loss_NN_RN_RN, eval_acc_NN_RN_RN = model_classif_trained_on_RN.evaluate(vect_RN_test_ml_eval,
                                                                                 label_test_ml_eval, verbose=2)
    eval_loss_NN_RN_gudhi, eval_acc_NN_RN_gudhi = model_classif_trained_on_RN.evaluate(vect_gudhi_test_ml_eval,
                                                                                 label_test_ml_eval, verbose=2)
    eval_loss_NN_RN_clean_gudhi, eval_acc_NN_RN_clean_gudhi = model_classif_trained_on_RN.evaluate(
        vect_gudhi_test_ml_eval_clean,
        label_test_ml_eval, verbose=2)
    eval_loss_NN_RN_clean_RN, eval_acc_NN_RN_clean_RN = model_classif_trained_on_RN.evaluate(
        vect_RN_test_ml_eval_clean,
        label_test_ml_eval, verbose=2)

    print('Test accuracy of NN trained on RipsNet vectorization, evaluated on clean Gudhi vectorization: ',
          eval_acc_NN_RN_clean_gudhi, '\n')
    print('Test accuracy of NN trained on RipsNet vectorization, evaluated on clean RN vectorization: ',
          eval_acc_NN_RN_clean_RN, '\n')
    print('\nTest accuracy of NN trained on RipsNet vectorization evaluated on noisy RN vectorization :',
          eval_acc_NN_RN_RN)
    print('\nTest accuracy of NN trained on RipsNet vectorization evaluated on noisy Gudhi vectorization :',
          eval_acc_NN_RN_gudhi, '\n')



    ### XGB classifier:
    model_classif_xgb_trained_on_gudhi = xgb.XGBClassifier(eval_metric='logloss', use_label_encoder=False)
    model_classif_xgb_trained_on_gudhi.fit(vect_gudhi_train_ml_train, label_train_ml_train)
    eval_acc_xgb_gudhi_clean_gudhi = model_classif_xgb_trained_on_gudhi.score(vect_gudhi_test_ml_eval_clean,
                                                                              label_test_ml_eval)
    eval_acc_xgb_gudhi_gudhi = model_classif_xgb_trained_on_gudhi.score(vect_gudhi_test_ml_eval, label_test_ml_eval)
    eval_acc_xgb_gudhi_RN = model_classif_xgb_trained_on_gudhi.score(vect_RN_test_ml_eval, label_test_ml_eval)
    print('Test accuracy of XGB trained on Gudhi, evaluated on clean Gudhi vectorization: ',
          eval_acc_xgb_gudhi_clean_gudhi, '\n')
    print('Test accuracy XGB trained on Gudhi vect, evaluated on (noisy) Gudhi vect:', eval_acc_xgb_gudhi_gudhi, '\n')
    print('Test accuracy XGB trained on Gudhi vect, evaluated on (noisy) RN vect:', eval_acc_xgb_gudhi_RN, '\n')

    model_classif_xgb_trained_on_DTM = xgb.XGBClassifier(eval_metric='logloss', use_label_encoder=False)
    model_classif_xgb_trained_on_DTM.fit(vect_gudhi_train_ml_train_DTM, label_train_ml_train)
    eval_acc_xgb_DTM_clean_DTM = model_classif_xgb_trained_on_DTM.score(vect_gudhi_test_ml_eval_clean_DTM,
                                                                              label_test_ml_eval)
    eval_acc_xgb_DTM_DTM = model_classif_xgb_trained_on_DTM.score(vect_gudhi_test_ml_eval_DTM, label_test_ml_eval)
    eval_acc_xgb_DTM_RN = model_classif_xgb_trained_on_DTM.score(vect_RN_test_ml_eval, label_test_ml_eval)
    print('Test accuracy of XGB trained on DTM vect, evaluated on clean DTM vectorization: ',
          eval_acc_xgb_DTM_clean_DTM, '\n')
    print('Test accuracy XGB trained on DTM vect, evaluated on (noisy) DTM vect:', eval_acc_xgb_DTM_DTM, '\n')
    print('Test accuracy XGB trained on DTM vect, evaluated on (noisy) RN vect:', eval_acc_xgb_DTM_RN, '\n')

    model_classif_xgb_trained_on_RN = xgb.XGBClassifier(eval_metric='logloss', use_label_encoder=False)
    model_classif_xgb_trained_on_RN.fit(vect_RN_train_ml_train, label_train_ml_train)
    eval_acc_xgb_RN_clean_gudhi = model_classif_xgb_trained_on_RN.score(vect_gudhi_test_ml_eval_clean,
                                                                        label_test_ml_eval)
    eval_acc_xgb_RN_clean_RN = model_classif_xgb_trained_on_RN.score(vect_RN_test_ml_eval_clean,
                                                                        label_test_ml_eval)
    eval_acc_xgb_RN_RN = model_classif_xgb_trained_on_RN.score(vect_RN_test_ml_eval, label_test_ml_eval)
    eval_acc_xgb_RN_gudhi = model_classif_xgb_trained_on_RN.score(vect_gudhi_test_ml_eval, label_test_ml_eval)
    print('Test accuracy of XGB trained on RN vect, evaluated on clean Gudhi vect: ', eval_acc_xgb_RN_clean_gudhi, '\n')
    print('Test accuracy of XGB trained on RN vect, evaluated on clean RN vect: ', eval_acc_xgb_RN_clean_RN, '\n')
    print('Test accuracy of XGB trained on RipsNet vect, eval on noisy RN vect:', eval_acc_xgb_RN_RN, '\n')
    print('Test accuracy of XGB trained on RipsNet vect, eval on noisy Gudhi vect:', eval_acc_xgb_RN_gudhi, '\n')

##########################################################################

    results_dict = {
                    'representation': representation,
                    'normalization': int(normalize),
                    'num_classes': num_classes,
                    'data gen fold': fold,
                    'N_train_sets': N_sets_train,
                    'N_test_sets': N_sets_test,
                    'dropout': dropout,
                    'batch_size': batch_size,
                    'regularization': regularization,
                    'pct_noise': pct_noise,
                    'homdim': str(homdims).replace(',', " "),
                    'GUDHI_time': timeG,
                    'RN_time': timeRN,
                    'DTM_time': timeDTM,
                    'eval_acc_xgb_gudhi_gudhi': eval_acc_xgb_gudhi_gudhi,
                    'eval_acc_xgb_RN_RN': eval_acc_xgb_RN_RN,
                    'eval_acc_xgb_gudhi_clean_gudhi': eval_acc_xgb_gudhi_clean_gudhi,
                    'eval_acc_xgb_RN_clean_RN': eval_acc_xgb_RN_clean_RN,
                    'eval_acc_xgb_DTM_DTM': eval_acc_xgb_DTM_DTM,
                    'eval_acc_xgb_DTM_clean_DTM': eval_acc_xgb_DTM_clean_DTM,
                    'eval_acc_xgb_RN_gudhi': eval_acc_xgb_RN_gudhi,
                    'eval_acc_xgb_gudhi_RN': eval_acc_xgb_gudhi_RN,
                    'eval_acc_xgb_RN_clean_gudhi': eval_acc_xgb_RN_clean_gudhi,
                    'eval_acc_NN_gudhi_gudhi': eval_acc_NN_gudhi_gudhi,
                    'eval_acc_NN_RN_RN': eval_acc_NN_RN_RN,
                    'eval_acc_NN_RN_gudhi': eval_acc_NN_RN_gudhi,
                    'eval_acc_NN_gudhi_RN': eval_acc_NN_gudhi_RN,
                    'eval_acc_NN_gudhi_clean_gudhi': eval_acc_NN_gudhi_clean_gudhi,
                    'eval_acc_NN_RN_clean_RN': eval_acc_NN_RN_clean_RN,
                    'eval_acc_NN_DTM_DTM': eval_acc_NN_DTM_DTM,
                    'eval_acc_NN_DTM_clean_DTM': eval_acc_NN_DTM_clean_DTM,
                    }
    if not model_net:
        results_dict.update({
            'TS_name': time_series_name,
            'TS length': len_TS,
            'skip': skip,
            'skipauto': skipauto,
            'delay': delay,
            'RC_max_dist': maxd,
            'tde dim': tdedim,
        })
    if model_net:
        results_dict.update({
            'normalize_pc': normalize_pc,
            'sample_even': sample_even,
        })

    if representation == 'PI':
        results_dict.update({
            'PI_size': PI_size,
            'bandwidth': vect_params_rn_train['bandwidth'],
            'scl_fct': scaling_factor,
            'weight': wgt_name,
        })
    if representation == 'LS':
        results_dict.update({
            'num_landscapes': num_landscapes,
            'resolution': resolution,
        })

    results_df = pd.DataFrame(results_dict, index=[0])
    add_header = False
    if not os.path.isfile(results_file):
        add_header = True
    results_df.to_csv(results_file, mode='a', header=add_header)

    plt.close('all')